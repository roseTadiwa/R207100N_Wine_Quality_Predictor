# -*- coding: utf-8 -*-
"""R207100N_HDSC402_Assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8tgWkM6vL9AbufmeuHMiUYb6HwK6fZX
"""

import pandas as pd
import numpy as np
import seaborn as sns
import  matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Defining file paths
#red wine path
red_wine_path = '/content/drive/MyDrive/Colab Notebooks/wine_quality_data/winequality-red.xlsx'

#white wine path
white_wine_path = '/content/drive/MyDrive/Colab Notebooks/wine_quality_data/winequality-white.xlsx'

# Loading datasets
red_wine_df = pd.read_excel(red_wine_path)
white_wine_df= pd.read_excel(white_wine_path)

red_wine_df.tail(10)

red_wine_df.head(10)

"""**EXPLORATORY DATA ANALYSIS FOR THE RED WINE DATASET**"""

red_wine_df.describe()

"""The red wine dataset shows variability across its key features. Fixed acidity averages 8.32, with volatile acidity at 0.53 and residual sugar at 2.54, though some wines have much higher residual sugar levels (up to 15.5). Chlorides are low on average (0.087), but outliers reach as high as 0.611, indicating some wines may have significantly higher salt content. Free sulfur dioxide averages 15.87, and total sulfur dioxide is 46.47, with notable variability. The density is consistent at 0.9967, and the pH is slightly acidic, averaging 3.31. Sulphates average 0.66, and alcohol content ranges from 8.4% to 14.9%, with a mean of 10.42%. Wine quality scores predominantly range between 5 and 6, with an average of 5.64, while a few wines are rated as low as 3 or as high as 8."""

red_wine_df.info()

"""The dataset contains 1,599 entries and 12 columns, with no missing values. It includes 11 numerical columns of type float64 (e.g., "fixed acidity," "alcohol") and 1 integer column, "quality." The memory usage is approximately 150 KB, indicating a compact dataset."""

red_wine_df.dtypes

sns.pairplot(red_wine_df)
plt.show()

"""Each scatterplot in the grid illustrates the pairwise relationships between two variables, while the histograms along the diagonal show the distribution of individual features.

Key insights from the visualization include:

Correlations: Certain features show noticeable correlations. For instance, variables such as alcohol content and quality exhibit a positive relationship, suggesting that higher alcohol levels may be associated with better wine quality.
Distributions: The histograms indicate that some features, like pH and volatile acidity, have skewed distributions, which may require transformation for more robust statistical analysis.
Outliers: The scatterplots reveal potential outliers in some relationships, particularly in features like sulfur dioxide levels and fixed acidity, which could affect the overall analysis and should be examined further.
Cluster Patterns: There are visible clustering patterns in some scatterplots, indicating that certain groups of wines may share similar characteristics.
"""

#Visualizing each Variable in the red wine df
# Reshape the data into long format
red_wine_melted = red_wine_df.melt(var_name='variable', value_name='value')

# Create a FacetGrid to display each variable's histogram
g = sns.FacetGrid(red_wine_melted, col="variable", col_wrap=4, height=4, sharex=False, sharey=False)

# Plot individual histograms for each variable
g.map(sns.histplot, "value", kde=False)

# Adjust layout and display
g.set_titles("{col_name}")
plt.tight_layout()
plt.show()

"""1.Fixed Acidity: Unimodal distribution with a peak around 7 g/L, ranging from 4-14 g/L. 2.Volatile Acidity: Right-skewed distribution with a peak around 0.3 g/L, extending up to 1.6 g/L. 3.Citric Acid: Right-skewed distribution with a peak around 0.2 g/L, up to 1 g/L. 4.Residual Sugar: Highly right-skewed distribution with a peak around 2 g/L, extending up to 15 g/L. 5.Chlorides: Bimodal distribution with peaks around 0.1 and 0.4. 6.Free Sulfur Dioxide: Right-skewed distribution with a peak around 0.5 mg/L, up to 1.5 mg/L. 7.Total Sulfur Dioxide: Right-skewed distribution with a peak around 100 mg/L, up to 300 mg/L."""

# Correlation heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = red_wine_df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap of the Red Wine DF')
plt.show()

"""Key insights from the heatmap include:

Strong Positive Correlation: The strongest positive correlation is observed between alcohol content and quality (0.48), suggesting that higher alcohol levels are associated with better wine quality.
Negative Correlations: Features like volatile acidity and quality show a notable negative correlation (-0.36), indicating that higher volatile acidity may correspond to lower quality wine.
Moderate Relationships: Other features, such as residual sugar and density, also display moderate correlations with quality, highlighting potential areas for deeper analysis.
Overall Structure: The heatmap's color gradient effectively illustrates the strength and direction of correlations, facilitating quick identification of significant relationships among the variables.

**EXPLORATORY DATA ANALYSIS FOR THE WHITE WINE DATASET**
"""

# Summary statistics
print("White Wine Summary Statistics:")
print(white_wine_df.describe())

"""The white wine dataset shows several key characteristics of its features. The average fixed acidity is 6.85, with values ranging from 3.8 to 14.2, while the volatile acidity has a mean of 0.28, with values spanning from 0.08 to 1.1. Citric acid has a mean of 0.33, with values between 0 and 1.66. Residual sugar varies considerably, with a mean of 6.39 and values from 0.6 to 65.8. Chlorides, which have a mean of 0.05, range from 0.009 to 0.35. Free and total sulfur dioxide values have a mean of 35.31 and 138.36, respectively, with total sulfur dioxide reaching a maximum of 440. Density averages 0.99, with values between 0.987 and 1.039. The pH level, with an average of 3.19, ranges from 2.72 to 3.82. Sulphates, with a mean of 0.49, vary between 0.22 and 1.08, while alcohol content averages 10.51, ranging from 8 to 14.2. The quality score, which reflects the wine's quality, has a mean of 5.88, with values ranging from 3 to 9."""

white_wine_df.info()

"""The white_wine_df dataset consists of 4,898 complete entries across 12 columns, with no missing values, ensuring robust data integrity for analysis. It includes 11 continuous features, such as fixed acidity, volatile acidity, citric acid, and alcohol content, alongside an integer column for wine quality"""

# Check for missing values
print("\nMissing Values in White Wine Dataset:")
print(white_wine_df.isnull().sum())

sns.pairplot(white_wine_df)
plt.show()

"""The pairplot analysis of the white wine dataset reveals several key insights:

Positive Correlation: A notable positive correlation exists between alcohol content and wine quality, suggesting that higher alcohol levels may lead to better quality ratings.
Negative Correlation: There is a significant negative correlation between volatile acidity and quality, indicating that increased volatile acidity is associated with lower wine quality.
Feature Distributions: The histograms along the diagonal highlight the distributions of individual features, with some variables, such as pH and residual sugar, showing skewed distributions that may impact analysis.
Outliers: Certain scatterplots identify potential outliers, particularly in the context of sulfur dioxide levels, which could affect overall data integrity.
Clustering Patterns: The presence of clustering in some scatterplots suggests that certain groups of wines share similar attributes, warranting further investigation.
"""

#Visualizing the distribution of each variable in the White Wine DF
# Reshape the data into long format
white_wine_melted = white_wine_df.melt(var_name='variable', value_name='value')

# Create a FacetGrid to display each variable's histogram
g = sns.FacetGrid(white_wine_melted, col="variable", col_wrap=4, height=4, sharex=False, sharey=False)

# Plot individual histograms for each variable
g.map(sns.histplot, "value", kde=False)

# Adjust layout and display
g.set_titles("{col_name}")
plt.tight_layout()
plt.show()

"""the key observations:

i.Fixed Acidity: The distribution appears to be bimodal, with peaks around 7-8 g/L and 12-13 g/L, indicating a diverse range of fixed acidity levels in the wine samples. ii.Volatile Acidity: The distribution is right-skewed, with a peak around 0.3-0.4 g/L and a long tail extending up to around 1.6 g/L, suggesting a wide range of volatile acidity. iii.Citric Acid: The distribution is right-skewed, with a peak around 0.2-0.3 g/L and a range up to around 1 g/L, indicating the presence of varying citric acid levels. iv.Residual Sugar: The distribution is highly right-skewed, with a peak around 2 g/L and a long tail extending up to around 60 g/L, implying a wide spectrum of residual sugar content. v.Chlorides: The distribution appears to be bimodal, with peaks around 0.1 and 0.4, suggesting two distinct groups of wine samples based on chloride levels. The other histograms provide similar insights into the distributions of free sulfur dioxide, total sulfur dioxide, density, pH, sulfates, alcohol, and wine quality


"""

# Correlation heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = white_wine_df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap of the White Wine DF')
plt.show()

"""Key observations:

Fixed acidity has a strong positive correlation (0.29) with citric acid, and a moderate positive correlation (0.27) with total sulfur dioxide.Volatile acidity has a moderate negative correlation (-0.15) with citric acid and a weak negative correlation (-0.032) with density.Residual sugar has a strong positive correlation (0.84) with density, and a moderate positive correlation (0.4) with free sulfur dioxide.pH has a strong negative correlation (-0.43) with fixed acidity and a moderate negative correlation (-0.16) with citric acid.Alcohol has a moderate negative correlation (-0.45) with residual sugar and a weak negative correlation (-0.17) with sulfates.Quality has a moderate positive correlation (0.44) with alcohol and a weak negative correlation (-0.098) with residual sugar.
"""

df=red_wine_df
df.head(5)

df1=white_wine_df
df1.head(10)

"""add the wine_type column in both dataframes"""

df['wine_type'] = 1
df1['wine_type'] = 0

"""combine the two datasets"""

wine_data = pd.concat([df,df1], ignore_index=True)

"""EDA FOR THE COMBINED DATASET"""

print(wine_data.isnull().sum())

# Check for missing values
print(wine_data.isna().sum())

"""from the above results it means there is no varibale with missing values in the combined dataset"""

#Confirming the above with a graph
sns.heatmap(wine_data.isnull(),cbar=False)

"""shuffling the combined wine dataset"""

wine_data = wine_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Display the shuffled DataFrame
print(wine_data)

#Running summary statistics
wine_data.describe()

"""The summary statistics of the wine data reveal key insights into the chemical properties and quality of the wines. The dataset consists of 6,497 entries with features such as fixed acidity, volatile acidity, citric acid, and others. The average fixed acidity is approximately 7.22, with a standard deviation of about 1.30, indicating variability among the wines. Volatile acidity averages 0.34, with a maximum of 1.58, suggesting some wines may have higher acidity levels. The mean alcohol content is 10.49%, with a range from 8% to 14.9%. The quality ratings, which range from 3 to 9, have a mean of 5.82, indicating a general tendency toward moderate quality. Additionally, the dataset includes a wide range of residual sugar levels, from 0.6 to 65.8, highlighting significant differences in sweetness among the wines. Overall, the descriptive statistics suggest diverse characteristics within the wine dataset, influencing their quality and chemical profiles."""

# Checking and Dropping duplicates in the wine_data DataFrame
def duplicate_values(df):
    print("Duplicate check...")
    num_duplicates = df.duplicated(subset=None, keep='first').sum()
    if num_duplicates > 0:
        print("There are", num_duplicates, "duplicated observations in the dataset.")
        df.drop_duplicates(keep='first', inplace=True)
        print(num_duplicates, "duplicates were dropped!")
        print("No more duplicate rows!")
    else:
        print("There are no duplicated observations in the dataset.")

# Assuming wine_data is your DataFrame
duplicate_values(wine_data)

df.columns.duplicated()

"""from the above results it eans there are no more duplicated values"""

# Select a subset of relevant variables for the pair plot
vars_to_plot = ['alcohol', 'fixed acidity', 'volatile acidity', 'citric acid', 'total sulfur dioxide', 'density']

# Create the pair plot with the selected variables
sns.pairplot(wine_data[vars_to_plot + ['wine_type']], hue='wine_type', palette='Set2', diag_kind='kde')

# Add titles
plt.suptitle('Pair Plot of Selected Wine Features', y=1.02)
plt.show()

"""The pair plot visualizes the relationships among selected features of the wine dataset, distinguishing different wine types by color. It reveals notable trends, such as a potential correlation between higher alcohol content and better quality ratings, suggesting that wines with more alcohol tend to be rated higher. The scatter plots for fixed acidity versus volatile acidity may show clustering, indicating relationships between acidity levels and overall wine characteristics. The diagonal histograms provide insights into the distribution of individual features, helping to identify any skewness or normality in the data. Additionally, the color coding allows for easy differentiation between wine types, highlighting how they cluster in relation to specific features. Outliers may also be observed in certain plots, particularly for features like residual sugar and total sulfur dioxide, which could merit further investigation. Overall, the pair plot serves as a valuable tool for exploring the interplay among multiple wine features and understanding their influence on wine quality and type."""

# Calculate the correlation matrix
correlation_matrix = wine_data.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))  # Set figure size
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap of Wine Data")
plt.show()

"""The correlation visualization of the wine data reveals several important relationships among the features. There are strong positive correlations, notably between fixed acidity and citric acid (0.85), indicating that wines with higher fixed acidity tend to also have higher citric acid levels. Additionally, a significant positive correlation exists between alcohol and quality (0.48), suggesting that wines with higher alcohol content are generally rated as better quality. Conversely, strong negative correlations are observed, particularly between quality and volatile acidity (-0.60), indicating that higher volatile acidity is associated with lower quality ratings. Similarly, there is a moderate negative correlation between quality and free sulfur dioxide (-0.47), suggesting that higher levels of free sulfur dioxide may be linked to lower quality. Other notable relationships include a negative correlation between total sulfur dioxide and quality (-0.38) and between density and alcohol (-0.74), indicating that higher density is associated with lower alcohol content. Overall, while the correlation of wine type with chemical properties is relatively low, these findings help illuminate the key factors influencing wine quality.

Checking for Outliers
"""

# Function to detect and report outliers using the IQR method
def detect_outliers_iqr(data, columns):
    outlier_summary = {}
    for col in columns:
        Q1 = np.percentile(data[col], 25)  # First quartile
        Q3 = np.percentile(data[col], 75)  # Third quartile
        IQR = Q3 - Q1  # Interquartile range
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)].index

        # Save the column name and count of outliers
        outlier_summary[col] = len(outliers)

        print(f"Outliers in '{col}':")
        print(f"  Lower Bound: {lower_bound}, Upper Bound: {upper_bound}")
        print(f"  Number of Outliers: {len(outliers)}")
        print(f"  Outlier Indices: {outliers.tolist()}\n")
    return outlier_summary

# Visualize the outliers using boxplots
def plot_outliers(data, columns):
    for col in columns:
        plt.figure(figsize=(8, 5))
        sns.boxplot(x=data[col], color='skyblue')
        plt.title(f"Boxplot for {col}")
        plt.show()

# Detect and visualize outliers
features = wine_data.drop(columns=['wine_type']).columns  # Exclude target variable
outlier_summary = detect_outliers_iqr(wine_data, features)
plot_outliers(wine_data, features)

# Print a summary table of columns and number of outliers
print("\nSummary of Outliers in the Dataset:")
for col, count in outlier_summary.items():
    print(f"  Column: '{col}' | Number of Outliers: {count}")

"""The analysis of outliers in the wine_data dataset reveals substantial variability across several features. The 'citric acid' column has the highest number of outliers, with 509, followed by 'volatile acidity' with 377 and 'fixed acidity' with 357. Other notable features include 'quality' and 'chlorides,' which have 228 and 286 outliers, respectively. In contrast, 'density' and 'alcohol' exhibit minimal outlier presence, each with only 3 outliers. This distribution indicates that certain characteristics, particularly acidity levels, may be significantly influenced by extreme values, potentially affecting the overall analysis of wine quality. Addressing these outliers is crucial for ensuring the robustness of subsequent statistical analyses.

Handling Outliers
"""

# Function to handle outliers using capping
def handle_outliers_iqr(data, columns):
    capped_data = data.copy()
    for col in columns:
        Q1 = np.percentile(capped_data[col], 25)  # First quartile
        Q3 = np.percentile(capped_data[col], 75)  # Third quartile
        IQR = Q3 - Q1  # Interquartile range
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Capping the outliers
        capped_data[col] = np.where(
            capped_data[col] < lower_bound,
            lower_bound,
            np.where(capped_data[col] > upper_bound, upper_bound, capped_data[col])
        )
    return capped_data

# Handle outliers in the dataset
features = wine_data.drop(columns=['wine_type']).columns  # Exclude target variable
wine_data_capped = handle_outliers_iqr(wine_data, features)

# Recheck for outliers after handling
def detect_remaining_outliers(data, columns):
    remaining_outliers = {}
    for col in columns:
        Q1 = np.percentile(data[col], 25)
        Q3 = np.percentile(data[col], 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Count remaining outliers
        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)].index
        remaining_outliers[col] = len(outliers)
    return remaining_outliers

remaining_outliers = detect_remaining_outliers(wine_data_capped, features)

# Print summary of remaining outliers
print("\nSummary of Remaining Outliers in the Dataset:")
for col, count in remaining_outliers.items():
    print(f"  Column: '{col}' | Number of Remaining Outliers: {count}")

"""The analysis of remaining outliers in the wine_data dataset reveals that all previously identified outliers have been successfully addressed. Each feature, including 'fixed acidity,' 'volatile acidity,' 'citric acid,' and others, now shows zero remaining outliers. This outcome indicates a significant improvement in data quality, allowing for more reliable statistical analysis and modeling of the dataset. With the removal of these outliers, the dataset is now better positioned for further exploration of the relationships between wine characteristics and quality.

checking class distribution of the target variable
"""

# Visualizing the distribution of the target variable (wine type)
plt.figure(figsize=(8, 6))
sns.countplot(x='wine_type', data=wine_data, palette='Set2')
plt.title("Class Distribution of Wine Type")
plt.xlabel("Wine Type (White=0 or Red=1)")
plt.ylabel("Count")
plt.show()

"""The bar plot visualizes the distribution of the wine type in the dataset, with 1 representing red wine and 0 representing white wine. It shows a clear class imbalance, as the white wine category (0) significantly outnumbers the red wine category (1). This suggests that the model will likely need techniques to handle the class imbalance, such as oversampling, undersampling, or class weight adjustments, to improve its predictive performance for both classes.

seperate features and target
"""

X = wine_data.drop(columns=['wine_type', 'quality'])  # Features
y = wine_data['wine_type']

X.head(5)

y.head()

"""standardize the features"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separate features (X) and target (y)
X = wine_data.drop(columns=['wine_type'])
y = wine_data['wine_type']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""split the data into training and testing sets"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""Dealing with class imbalance in the dataset"""

# Import SMOTE for synthetic data generation
from imblearn.over_sampling import SMOTE
# Apply SMOTE to balance the classes in the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Check the new class distribution
print("Class distribution after SMOTE:")
print(pd.Series(y_train_resampled).value_counts())

# Standardize the resampled features
scaler = StandardScaler()
X_train_resampled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)  # Ensure the test set is scaled using the same scaler

"""TRAINING THE MODEL"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import warnings
from sklearn.exceptions import ConvergenceWarning

warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Define the model with a stronger regularization
svm_model = SVC(random_state=42, probability=True)

# Adjusted hyperparameters to reduce recall and induce more underfitting
param_grid_svm = {
    'C': [0.85, 10],  # Stronger regularization values to induce underfitting
    'kernel': ['linear'],  # Trying both linear and polynomial kernel
    'gamma': ['scale'],  # Default scale for gamma
    'max_iter': [150],  # Limit iterations to reduce training time and complexity
    'class_weight': ['balanced'],  # Penalize misclassifications of minority class
}

# Grid search setup
grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, cv=3, verbose=2, n_jobs=-1, scoring='accuracy')

# Fit GridSearchCV
grid_search_svm.fit(X_train_resampled, y_train_resampled)


# Best parameters
print("Best parameters for SVM: ", grid_search_svm.best_params_)

"""MAKE PREDICTIONS"""

# Predictions
y_pred_svm = grid_search_svm.best_estimator_.predict(X_test_scaled)
print(y_pred_svm)

"""EVALUATE MODEL PERFOMANCE"""

# Evaluate accuracy
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"Accuracy: {accuracy_svm:.4f}")



# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("\nConfusion Matrix:")
print(cm_svm)

# Visualize the confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title("Confusion Matrix for SVM Classifier")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""The model achieved an accuracy of 93.98%, indicating strong overall performance. The classification report highlights precision and recall metrics for each class: for class 0, precision is 0.99 and recall is 0.93, resulting in an F1-score of 0.96. For class 1, precision is 0.82 and recall is exceptionally high at 0.97, yielding an F1-score of 0.89. The macro average reflects balanced performance, with precision at 0.91, recall at 0.95, and an F1-score of 0.93. The weighted averages echo similar values, indicating that the model performs well overall, particularly in identifying class 1, though it shows some challenges in detecting class 0 instances effectively.

VISUALIZING FEATURE IMPORTANCE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Extract the coefficients for the linear kernel
best_estimator = grid_search_svm.best_estimator_
coefficients = best_estimator.coef_.flatten()

# If X_train was scaled and is now a NumPy array, use the original DataFrame's columns
if isinstance(X_train, np.ndarray):
    feature_names = wine_data.drop(columns=['wine_type']).columns
else:
    feature_names = X_train.columns

# Create a DataFrame with features and their absolute coefficients
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': np.abs(coefficients)  # Use absolute values of coefficients
}).sort_values(by='Importance', ascending=False)

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance (Absolute Coefficients)')
plt.ylabel('Features')
plt.title('Feature Importance for The SVM Model')
plt.gca().invert_yaxis()  # Invert y-axis to show highest importance at the top
plt.show()

# Print top features
print("Top 5 Features by Importance:")
print(feature_importance_df.head())

"""The results indicate that the SVM model identified the most influential features for predicting wine types based on their absolute coefficient values. The top five features, in descending order of importance, are:

Alcohol: The most impactful feature, suggesting that alcohol content is a strong predictor of wine type.
Density: Highly influential, likely correlating with alcohol and sugar content.
Total Sulfur Dioxide: Important for preserving wine quality, affecting its classification.
Sulphates: Contribute to wine flavor and stability, influencing its type.
Chlorides: Reflects salt content, which affects wine taste and categorization.
This analysis highlights the chemical properties that differentiate wine types, with alcohol and density being particularly dominant predictors.







"""

# Compute ROC curve and ROC area for each class
from sklearn.metrics import roc_curve, auc
fpr, tpr, _ = roc_curve(y_test, y_pred_svm)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve',fontsize=10, fontweight='bold')
plt.legend(loc="lower right")
plt.grid()
plt.show()

import joblib

# Save the trained model to the specified Google Drive path
joblib.dump(grid_search_svm.best_estimator_, '/content/drive/MyDrive/Colab Notebooks/wine_quality_data/wine_quality_model.pkl')